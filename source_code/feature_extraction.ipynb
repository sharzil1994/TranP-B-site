{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48de5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6be383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharzil/anaconda3/envs/tf12/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:7\n"
     ]
    }
   ],
   "source": [
    "# from transformers import T5EncoderModel, T5Tokenizer \n",
    "from transformers import T5EncoderModel, T5Tokenizer ,AlbertModel, AlbertTokenizer,BertModel, BertTokenizer\n",
    "from transformers  import XLNetModel, XLNetTokenizer\n",
    "\n",
    "import torch\n",
    "import h5py\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2f71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alphabet(string, alphabet):\n",
    "    return alphabet in string\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    # Define dictionary mapping amino acids to their indices\n",
    "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    aa_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "    \n",
    "    # Initialize one-hot encoded sequence\n",
    "    one_hot_sequence = []\n",
    "    \n",
    "    # Iterate over each amino acid in the sequence\n",
    "    for aa in sequence:\n",
    "        # Initialize one-hot encoding vector for current amino acid\n",
    "        encoding = [0] * len(amino_acids)\n",
    "        # Set the index corresponding to the amino acid to 1\n",
    "        if find_alphabet(amino_acids, aa):\n",
    "            encoding[aa_to_index[aa]] = 1\n",
    "        # Append the one-hot encoding vector to the sequence\n",
    "        one_hot_sequence.append(encoding)\n",
    "    one_hot_sequence = np.array(one_hot_sequence)   \n",
    "    return one_hot_sequence\n",
    "\n",
    "def get_T5_model(device):\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_ProtBert(device):\n",
    "    model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_ProtT5_XL_BFD(device):\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\", do_lower_case=False )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_ProtXLNet(device):    \n",
    "    xlnet_men_len = 512\n",
    "    model = XLNetModel.from_pretrained(\"Rostlab/prot_xlnet\",mem_len=xlnet_men_len)\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = XLNetTokenizer.from_pretrained(\"Rostlab/prot_xlnet\", do_lower_case=False)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_ProtBert_BFD(device):\n",
    "    model = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings( model, tokenizer, seqs, per_residue, per_protein, sec_struct, \n",
    "                   max_residues=4000, max_seq_len=1000, max_batch=100 ):\n",
    "\n",
    "    if sec_struct:\n",
    "      sec_struct_model = load_sec_struct_model()\n",
    "\n",
    "    results = {\"residue_embs\" : dict(), \n",
    "               \"protein_embs\" : dict(),\n",
    "               \"sec_structs\" : dict() \n",
    "               }\n",
    "\n",
    "    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding)\n",
    "    seq_dict   = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n",
    "    start = time.time()\n",
    "    batch = list()\n",
    "    for seq_idx, (pdb_id, seq) in enumerate(seq_dict,1):\n",
    "        seq = seq\n",
    "        seq_len = len(seq)\n",
    "        seq = ' '.join(list(seq))\n",
    "        batch.append((pdb_id,seq,seq_len))\n",
    "\n",
    "        n_res_batch = sum([ s_len for  _, _, s_len in batch ]) + seq_len \n",
    "        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n",
    "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
    "            batch = list()\n",
    "\n",
    "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "            input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "            embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            for batch_idx, identifier in enumerate(pdb_ids): # for each protein in the current mini-batch\n",
    "                s_len = seq_lens[batch_idx]\n",
    "                emb = embedding_repr.last_hidden_state[batch_idx,:s_len]\n",
    "                if per_residue: # store per-residue embeddings (Lx1024)\n",
    "                    results[\"residue_embs\"][ identifier ] = emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    passed_time=time.time()-start\n",
    "\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_features(seq_all):\n",
    "\n",
    "\n",
    "\n",
    "    !mkdir protein_seqences\n",
    "    data4 = seq_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    temp= dict()\n",
    "    for keys in data4.keys():\n",
    "        if  len(data4[keys][0]) <=500:\n",
    "            temp[keys] = data4[keys][0]\n",
    "\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    test2 = dict()\n",
    "    handle = 0\n",
    "    for keys in temp.keys():\n",
    "        # print(keys)\n",
    "        handle = 0\n",
    "        test2[keys] = temp[keys]\n",
    "\n",
    "        if i >= 0:\n",
    "            i = 0\n",
    "            #/media/4TB_hardisk/sharzil/Downloads/alphapdb/dictionary_train/  this is output folder name\n",
    "            #create this folder \n",
    "            with open('./protein_seqences/ppisequence'+str(j)+'.pkl', 'wb') as file:\n",
    "                pickle.dump(test2, file)\n",
    "            j = j+1\n",
    "            handle = 1\n",
    "            test2 = dict()\n",
    "        i = i+1\n",
    "    if handle == 0:\n",
    "        with open('./protein_seqences/ppisequence'+str(j)+'.pkl', 'wb') as file:\n",
    "            pickle.dump(test2, file)\n",
    "\n",
    "    !mkdir protT5 # root directory for storing checkpoints, results etc\n",
    "    !mkdir protT5/protT5_checkpoint # directory holding the ProtT5 checkpoint\n",
    "    !mkdir protT5/sec_struct_checkpoint # directory storing the supervised classifier's checkpoint\n",
    "    !mkdir protT5/output # directory for storing your embeddings & predictions\n",
    "    !wget -nc -P protT5/ https://rostlab.org/~deepppi/example_seqs.fasta\n",
    "    # Huge kudos to the bio_embeddings team here! We will integrate the new encoder, half-prec ProtT5 checkpoint soon\n",
    "    !wget -nc -P protT5/sec_struct_checkpoint http://data.bioembeddings.com/public/embeddings/feature_models/t5/secstruct_checkpoint.pt\n",
    "\n",
    "\n",
    "    seq_path = \"./protT5/example_seqs.fasta\"\n",
    "\n",
    "    per_residue = True \n",
    "    per_residue_path = \"./protT5/output/per_residue_embeddings.h5\" # where to store the embeddings\n",
    "\n",
    "    per_protein = True\n",
    "    per_protein_path = \"./protT5/output/per_protein_embeddings.h5\" # where to store the embeddings\n",
    "\n",
    "    sec_struct = False\n",
    "    sec_struct_path = \"./protT5/output/ss3_preds.fasta\" # file for storing predictions\n",
    "\n",
    "    assert per_protein is True or per_residue is True or sec_struct is True, print(\n",
    "        \"Minimally, you need to active per_residue, per_protein or sec_struct. (or any combination)\")\n",
    "\n",
    "    model, tokenizer = get_T5_model(device)\n",
    "    # model, tokenizer = get_ProtBert(device)\n",
    "    # model, tokenizer = get_ProtT5_XL_BFD(device)\n",
    "    # model, tokenizer = get_ProtXLNet(device)\n",
    "    # model, tokenizer =get_ProtBert_BFD(device)\n",
    "\n",
    "    protein_feat_r=dict()\n",
    "    i=0\n",
    "    for index in seq_all.keys():\n",
    "        temp=dict()\n",
    "        temp[index]=seq_all[index]\n",
    "        results=get_embeddings( model, tokenizer, temp,\n",
    "                             per_residue, per_protein, sec_struct)\n",
    "        i=i+1\n",
    "        temp1=results['residue_embs']\n",
    "        for index2 in temp1.keys():\n",
    "            protein_feat_r[index2]=temp1[index2]\n",
    "        \n",
    "    protein_onehot=dict()\n",
    "\n",
    "    for index in seq_all.keys():\n",
    "        temp=seq_all[index]\n",
    "        encoded_sequence = one_hot_encode(temp)\n",
    "        protein_onehot[index]=encoded_sequence\n",
    "\n",
    "    return protein_feat_r,protein_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb3877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddb821a-8ac8-4d55-8c0d-571a46c05a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature loading and features extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77aa50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdata='/media/4TB_hardisk/sharzil/Downloads/Dataset/'\n",
    "\n",
    "# pathdata='./dataset/'\n",
    "with open(pathdata+'Train_335.pkl', 'rb') as file:\n",
    "    train = pickle.load(file) \n",
    "with open(pathdata+'Test_60.pkl', 'rb') as file:\n",
    "    test = pickle.load(file)\n",
    "with open(pathdata+'Test_20_new.pkl', 'rb') as file:\n",
    "    test_20 = pickle.load(file)  \n",
    "with open(pathdata+'UBtest_31.pkl', 'rb') as file:\n",
    "    test_UB_31 = pickle.load(file)\n",
    "with open(pathdata+'Btest_31.pkl', 'rb') as file:\n",
    "    test_B_31 = pickle.load(file)\n",
    "    \n",
    "    \n",
    "all_data=dict()\n",
    "for index in train.keys():\n",
    "    all_data[index]=train[index]\n",
    "for index in test.keys():\n",
    "    all_data[index]=test[index]\n",
    "for index in test_20.keys():\n",
    "    all_data[index]=test_20[index]\n",
    "for index in test_UB_31.keys():\n",
    "    all_data[index]=test_UB_31[index]\n",
    "for index in test_B_31.keys():\n",
    "    all_data[index]=test_B_31[index]\n",
    "\n",
    "seq_all=dict()\n",
    "for index in all_data.keys():\n",
    "    temp=all_data[index]\n",
    "    seq_all[index]=temp[0]\n",
    "label_all=dict()\n",
    "for index in all_data.keys():\n",
    "    temp=all_data[index]\n",
    "    label_all[index]=temp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a72764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘protein_seqences’: File exists\n",
      "mkdir: cannot create directory ‘protT5’: File exists\n",
      "mkdir: cannot create directory ‘protT5/protT5_checkpoint’: File exists\n",
      "mkdir: cannot create directory ‘protT5/sec_struct_checkpoint’: File exists\n",
      "mkdir: cannot create directory ‘protT5/output’: File exists\n",
      "File ‘protT5/example_seqs.fasta’ already there; not retrieving.\n",
      "\n",
      "File ‘protT5/sec_struct_checkpoint/secstruct_checkpoint.pt’ already there; not retrieving.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 361/361 [00:00<00:00, 114kB/s]\n",
      "Downloading: 100%|██████████| 1.57G/1.57G [01:16<00:00, 21.9MB/s] \n",
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 81.0/81.0 [00:00<00:00, 27.3kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 35.2kB/s]\n",
      "Downloading: 100%|██████████| 86.0/86.0 [00:00<00:00, 20.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "result1, result2=get_features(seq_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58957a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_folder=('./feature_save')\n",
    "with open(file=os.path.join(out_folder,'protein_feat_r.pkl'), mode='wb') as f:\n",
    "    pickle.dump(result1, f)\n",
    "with open(file=os.path.join(out_folder,'protein_feat_one.pkl'), mode='wb') as f:\n",
    "    pickle.dump(result2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df94ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdata='./pisite_dataset/'\n",
    "\n",
    "with open(pathdata+'Test_70.pkl', 'rb') as file:\n",
    "    test_70 = pickle.load(file) \n",
    "        \n",
    "all_data=dict()\n",
    "for index in test_70.keys():\n",
    "    all_data[index]=test_70[index]\n",
    "seq_all=dict()\n",
    "for index in all_data.keys():\n",
    "    temp=all_data[index]\n",
    "    seq_all[index]=temp[0]\n",
    "label_all=dict()\n",
    "for index in all_data.keys():\n",
    "    temp=all_data[index]\n",
    "    label_all[index]=temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94504cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c77f6e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘protein_seqences’: File exists\n",
      "mkdir: cannot create directory ‘protT5’: File exists\n",
      "mkdir: cannot create directory ‘protT5/protT5_checkpoint’: File exists\n",
      "mkdir: cannot create directory ‘protT5/sec_struct_checkpoint’: File exists\n",
      "mkdir: cannot create directory ‘protT5/output’: File exists\n",
      "File ‘protT5/example_seqs.fasta’ already there; not retrieving.\n",
      "\n",
      "File ‘protT5/sec_struct_checkpoint/secstruct_checkpoint.pt’ already there; not retrieving.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "result1, result2=get_features(seq_all)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a4bfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_folder=('./feature_save')\n",
    "with open(file=os.path.join(out_folder,'protein_feat_pisite_r.pkl'), mode='wb') as f:\n",
    "    pickle.dump(result1, f)\n",
    "with open(file=os.path.join(out_folder,'protein_feat_one.pkl'), mode='wb') as f:\n",
    "    pickle.dump(result2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1620773",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
